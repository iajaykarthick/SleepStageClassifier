{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuition Behind Independent Component Analysis (ICA)\n",
    "\n",
    "Independent Component Analysis (ICA) is a powerful computational technique used for separating a multivariate signal into its additive, independent non-Gaussian signals. This method is especially useful in the analysis of EEG (electroencephalogram), EMG (electromyogram), and EOG (electrooculogram) data, where it can be applied to identify and remove artifacts such as eye blinks, cardiac signals, or muscle activity. Below is the intuition behind how ICA achieves this:\n",
    "\n",
    "### Non-Gaussian Sources\n",
    "\n",
    "- ICA operates under the assumption that the source signals are **non-Gaussian** and **statistically independent**. Non-Gaussian signals are chosen because Gaussian distributions are fully described by their mean and variance, and a mixture of Gaussian signals would result in another Gaussian distribution, making it difficult to separate the sources. Non-Gaussian signals, on the other hand, provide more information through higher-order statistics, which ICA can exploit to distinguish between them.\n",
    "\n",
    "### Statistical Independence\n",
    "\n",
    "- The goal of ICA is to decompose a multivariate signal into components that are statistically independent of each other. Statistical independence means that the information provided by one signal does not affect the information provided by another. This property is key to separating out artifacts from signals like EEG, as these artifacts are often unrelated to the brain activity being measured and can be considered independent.\n",
    "\n",
    "### Central Limit Theorem (CLT)\n",
    "\n",
    "- The Central Limit Theorem plays a significant role in ICA. It states that the sum (or average) of a large number of independent, identically distributed variables tends towards a Gaussian distribution, regardless of the original variables' distribution. In ICA, this implies that while the mixed signals may appear more Gaussian due to the CLT, the original source signals are more non-Gaussian. ICA leverages this by assuming that the observed mixtures are closer to Gaussian distributions and seeks to maximize the non-Gaussianity of the components to find the original sources.\n",
    "\n",
    "### Maximizing Non-Gaussianity\n",
    "\n",
    "- ICA involves algorithms that aim to maximize the non-Gaussianity of the output signals. This maximization is typically quantified using statistical measures such as negentropy or kurtosis. \n",
    "- The intuition is that by identifying a transformation that maximizes the non-Gaussian characteristics of the output, ICA can effectively isolate the original, independent source signals from their mixtures.\n",
    "\n",
    "### Application to EEG/EMG/EOG\n",
    "\n",
    "- In the context of EEG, EMG, and EOG signals, the sources of interest (e.g., brain activity, muscle activity, eye movements) are mixed within the observed signals. These sources are assumed to be independent (eye movements are not dependent on cognitive task-related brain activity, for instance) and non-Gaussian. By exploiting these properties, ICA can isolate and remove artifacts, leading to cleaner signals for further analysis and interpretation.\n",
    "\n",
    "The intuition behind ICA is centered on its capability to utilize the non-Gaussianity and statistical independence of source signals for effective separation from their mixtures, which is invaluable in biomedical signal processing for artifact removal and signal clarification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How PCA and ICA Differ:\n",
    "\n",
    "- Principal Component Analysis (PCA) is a statistical technique primarily used for dimensionality reduction by identifying principal components that capture the most variance in the data. \n",
    "- Independent Component Analysis (ICA), on the other hand, is mainly used for signal separation by identifying underlying statistically independent components from a mixture of signals. While ICA can be applied for dimensionality reduction in specific contexts, its primary application and strength lie in the separation of independent sources. \n",
    " \n",
    "Despite their similarities in application, PCA and ICA are fundamentally different in their objectives and methodologies. \n",
    "\n",
    "### Geometric Perspective\n",
    "\n",
    "**PCA**:\n",
    "- Geometrically, PCA identifies the directions (principal components) in the data space that maximize variance. These directions are orthogonal to each other and define a new coordinate system where the data can be projected.\n",
    "- Imagine the data points in a multidimensional space. PCA finds the axes in this space such that when the data is projected onto these axes, the variance along them is maximized. The first principal component has the highest variance, with each subsequent component having the next highest variance, under the constraint that it is orthogonal to the preceding components.\n",
    "\n",
    "**ICA**:\n",
    "- ICA, on the other hand, does not look for orthogonal components but for statistically independent components. The geometric intuition behind ICA is to rotate and stretch the coordinate axes in such a way that the projected data points on these axes are as statistically independent from each other as possible.\n",
    "- This means that while PCA's components are orthogonal lines or planes in the multidimensional space, ICA's components might not be orthogonal or aligned with the axes of maximum variance. Instead, they align with the underlying independent sources that generated the data.\n",
    "\n",
    "### Analytical Perspective\n",
    "\n",
    "**PCA**:\n",
    "- Mathematically, PCA is defined by the eigenvalue decomposition of the covariance matrix of the data or by singular value decomposition (SVD) of the data matrix itself.\n",
    "- Let $X$ be the data matrix. The covariance matrix $C$ is given by $C = \\frac{1}{n-1}X^TX$ (assuming $X$ is mean-centered). PCA finds the eigenvalues and eigenvectors of $C$, where the eigenvectors form the principal components, and the eigenvalues indicate the variance captured by each component.\n",
    "- The principal components $P$ of $X$ can then be found as $P = XE$, where $E$ is the matrix of eigenvectors of $C$.\n",
    "\n",
    "**ICA**:\n",
    "- ICA, in contrast, is based on the assumption that the observed data are linear mixtures of some unknown latent variables (the independent components), and the goal is to recover these components.\n",
    "- Mathematically, if $X$ is the observed data matrix, ICA models $X$ as $X = AS$, where $A$ is the mixing matrix, and $S$ represents the matrix of independent components. The aim is to estimate the unmixing matrix $W$ such that $S = WX$.\n",
    "- ICA uses optimization techniques to maximize the statistical independence of the components in $S$, often leveraging measures of non-Gaussianity such as negentropy or kurtosis, since independence implies non-Gaussianity under the Central Limit Theorem.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "While PCA focuses on maximizing variance and finding orthogonal components that represent the data, ICA seeks to uncover the original, statistically independent sources that generated the observed dataset. Geometrically, PCA works by identifying the most significant orthogonal directions, whereas ICA aims to find a coordinate system where the projections of the data points are statistically independent. Analytically, PCA utilizes the covariance matrix and its eigen decomposition, while ICA models the data as mixtures of independent components and employs optimization methods to maximize their statistical independence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecg_proj_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
